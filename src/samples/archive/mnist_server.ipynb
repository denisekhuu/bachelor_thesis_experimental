{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381fa028-ac8d-48bb-9635-216c009c6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_learning.utils import SHAPUtil\n",
    "from federated_learning import ClientPlane, Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e279c57-4bca-4e60-84d7-7276ccc58f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObserverConfiguration():\n",
    "    experiment_type = \"shap_clean_run\"\n",
    "    experiment_id = 0\n",
    "    test = True\n",
    "    dataset_type = \"MNIST\"\n",
    "    \n",
    "    # Client Configurations \n",
    "    client_name = \"client\"\n",
    "    client_type = \"client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1257b9-be96-4362-8ac0-508bc2081836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training data loaded.\n",
      "MNIST test data loaded.\n"
     ]
    }
   ],
   "source": [
    "config = Configuration()\n",
    "data = config.DATASET(config)\n",
    "shap_util = SHAPUtil(data.test_dataloader)\n",
    "observer_config = ObserverConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a21a5b-7258-4d0e-8e95-9aee458f06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAggregator():\n",
    "    def model_avg(self, parameters):\n",
    "        new_params = {}\n",
    "        for name in parameters[0].keys():\n",
    "            new_params[name] = sum([param[name].data for param in parameters]) / len(parameters)\n",
    "        return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68034e08-f996-4e36-a187-a53c9e00cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "class ClientSelector():\n",
    "    def random_selector(self, number_of_clients, clients_per_round):\n",
    "        rng = default_rng()\n",
    "        return rng.choice(number_of_clients, size=clients_per_round, replace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6caa93b7-8dab-482d-b436-3eb4432995bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "class Server():\n",
    "    def __init__(self, config, shap_util):\n",
    "        self.config = config\n",
    "        self.default_model_path = os.path.join(self.config.TEMP, 'models', \"{}.model\".format(self.config.MODELNAME))\n",
    "        self.net = self.load_default_model()\n",
    "        self.aggregator = ModelAggregator()\n",
    "        self.selector = ClientSelector()\n",
    "        self.shap_util = shap_util\n",
    "        self.rounds = 0\n",
    "        self.e = []\n",
    "    \n",
    "    def set_rounds(self, rounds):\n",
    "        self.rounds = rounds\n",
    "        \n",
    "    def create_default_model(self):\n",
    "        Path(os.path.dirname(self.default_model_path)).mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.net.state_dict(), self.default_model_path)\n",
    "        print(\"default model saved to:{}\".format(os.path.dirname(self.default_model_path)))\n",
    "    \n",
    "    def load_default_model(self):\n",
    "        \"\"\"\n",
    "        Load a model from a file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.default_model_path):\n",
    "            self.create_default_model()\n",
    "        if os.path.exists(self.default_model_path):\n",
    "            try:\n",
    "                model = self.config.NETWORK()\n",
    "                model.load_state_dict(torch.load(self.default_model_path))\n",
    "                model.eval()\n",
    "                print(\"Load model successfully\")\n",
    "            except:\n",
    "                print(\"Couldn't load model\")\n",
    "        else:\n",
    "            print(\"Could not find model: {}\".format(self.default_model_path))   \n",
    "        return model\n",
    "            \n",
    "    def get_nn_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the NN's parameters.\n",
    "        \"\"\"\n",
    "        return self.net.state_dict()\n",
    "    \n",
    "    def update_nn_parameters(self, new_params):\n",
    "        \"\"\"\n",
    "        Update the NN's parameters.\n",
    "\n",
    "        :param new_params: New weights for the neural network\n",
    "        :type new_params: dict\n",
    "        \"\"\"\n",
    "        self.net.load_state_dict(new_params, strict=True)\n",
    "        \n",
    "    def select_clients(self):\n",
    "        return self.selector.random_selector(self.config.NUMBER_OF_CLIENTS, self.config.CLIENTS_PER_ROUND)\n",
    "\n",
    "    def aggregate_model(self, client_parameters): \n",
    "        new_parameters = self.aggregator.model_avg(client_parameters)\n",
    "        self.update_nn_parameters(new_parameters)\n",
    "        if (self.rounds + 1)%50 == 0:\n",
    "            print(\"Model aggregation in round {} was successful\".format(self.rounds+1))\n",
    "        \n",
    "    def get_shap_values(self):\n",
    "        \"\"\"\n",
    "        Calculate SHAP values and SHAP image predictions \n",
    "        \"\"\"\n",
    "        if not self.e: \n",
    "            self.e = self.shap_util.set_deep_explainer(self.net)\n",
    "        self.shap_values = self.shap_util.get_shap_values(self.e)\n",
    "        self.shap_prediction = self.shap_util.predict(self.net)\n",
    "    \n",
    "    def set_explainer(self): \n",
    "        self.e = self.shap_util.deep_explainer(self.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "215ea7e6-0839-4402-b7f5-1da7662f1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model successfully\n",
      "Create 200 clients with dataset of size 300\n",
      "Model aggregation in round 49 was successful\n",
      "Train Epoch: 99 [0/300 (0%)]\tLoss: 0.420564\n",
      "Train Epoch: 99 [10/300 (3%)]\tLoss: 0.057684\n",
      "Train Epoch: 99 [20/300 (7%)]\tLoss: 0.085317\n",
      "Train Epoch: 99 [30/300 (10%)]\tLoss: 0.041510\n",
      "Train Epoch: 99 [40/300 (13%)]\tLoss: 1.133566\n",
      "Train Epoch: 99 [50/300 (17%)]\tLoss: 0.856403\n",
      "Train Epoch: 99 [60/300 (20%)]\tLoss: 0.647120\n",
      "Train Epoch: 99 [70/300 (23%)]\tLoss: 0.156760\n",
      "Train Epoch: 99 [80/300 (27%)]\tLoss: 0.113244\n",
      "Train Epoch: 99 [90/300 (30%)]\tLoss: 0.587460\n",
      "Train Epoch: 99 [100/300 (33%)]\tLoss: 0.487534\n",
      "Train Epoch: 99 [110/300 (37%)]\tLoss: 0.133908\n",
      "Train Epoch: 99 [120/300 (40%)]\tLoss: 0.464023\n",
      "Train Epoch: 99 [130/300 (43%)]\tLoss: 0.160590\n",
      "Train Epoch: 99 [140/300 (47%)]\tLoss: 0.140401\n",
      "Train Epoch: 99 [150/300 (50%)]\tLoss: 0.662591\n",
      "Train Epoch: 99 [160/300 (53%)]\tLoss: 1.293758\n",
      "Train Epoch: 99 [170/300 (57%)]\tLoss: 1.039700\n",
      "Train Epoch: 99 [180/300 (60%)]\tLoss: 0.315818\n",
      "Train Epoch: 99 [190/300 (63%)]\tLoss: 0.512683\n",
      "Train Epoch: 99 [200/300 (67%)]\tLoss: 0.387742\n",
      "Train Epoch: 99 [210/300 (70%)]\tLoss: 0.185829\n",
      "Train Epoch: 99 [220/300 (73%)]\tLoss: 0.045205\n",
      "Train Epoch: 99 [230/300 (77%)]\tLoss: 0.984838\n",
      "Train Epoch: 99 [240/300 (80%)]\tLoss: 0.179108\n",
      "Train Epoch: 99 [250/300 (83%)]\tLoss: 0.430049\n",
      "Train Epoch: 99 [260/300 (87%)]\tLoss: 0.463943\n",
      "Train Epoch: 99 [270/300 (90%)]\tLoss: 0.057953\n",
      "Train Epoch: 99 [280/300 (93%)]\tLoss: 0.374062\n",
      "Train Epoch: 99 [290/300 (97%)]\tLoss: 0.388962\n",
      "Train Epoch: 99 [0/300 (0%)]\tLoss: 0.502110\n",
      "Train Epoch: 99 [10/300 (3%)]\tLoss: 0.428219\n",
      "Train Epoch: 99 [20/300 (7%)]\tLoss: 0.045730\n",
      "Train Epoch: 99 [30/300 (10%)]\tLoss: 0.349417\n",
      "Train Epoch: 99 [40/300 (13%)]\tLoss: 0.147234\n",
      "Train Epoch: 99 [50/300 (17%)]\tLoss: 0.028644\n",
      "Train Epoch: 99 [60/300 (20%)]\tLoss: 0.097507\n",
      "Train Epoch: 99 [70/300 (23%)]\tLoss: 0.142323\n",
      "Train Epoch: 99 [80/300 (27%)]\tLoss: 0.266103\n",
      "Train Epoch: 99 [90/300 (30%)]\tLoss: 0.114270\n",
      "Train Epoch: 99 [100/300 (33%)]\tLoss: 0.162223\n",
      "Train Epoch: 99 [110/300 (37%)]\tLoss: 0.274886\n",
      "Train Epoch: 99 [120/300 (40%)]\tLoss: 0.396430\n",
      "Train Epoch: 99 [130/300 (43%)]\tLoss: 0.141079\n",
      "Train Epoch: 99 [140/300 (47%)]\tLoss: 0.090790\n",
      "Train Epoch: 99 [150/300 (50%)]\tLoss: 0.728927\n",
      "Train Epoch: 99 [160/300 (53%)]\tLoss: 0.475778\n",
      "Train Epoch: 99 [170/300 (57%)]\tLoss: 0.880870\n",
      "Train Epoch: 99 [180/300 (60%)]\tLoss: 0.040132\n",
      "Train Epoch: 99 [190/300 (63%)]\tLoss: 0.450972\n",
      "Train Epoch: 99 [200/300 (67%)]\tLoss: 0.752234\n",
      "Train Epoch: 99 [210/300 (70%)]\tLoss: 0.298196\n",
      "Train Epoch: 99 [220/300 (73%)]\tLoss: 0.637979\n",
      "Train Epoch: 99 [230/300 (77%)]\tLoss: 0.296063\n",
      "Train Epoch: 99 [240/300 (80%)]\tLoss: 0.348890\n",
      "Train Epoch: 99 [250/300 (83%)]\tLoss: 0.247762\n",
      "Train Epoch: 99 [260/300 (87%)]\tLoss: 0.052311\n",
      "Train Epoch: 99 [270/300 (90%)]\tLoss: 0.641665\n",
      "Train Epoch: 99 [280/300 (93%)]\tLoss: 0.045725\n",
      "Train Epoch: 99 [290/300 (97%)]\tLoss: 0.397033\n",
      "Train Epoch: 99 [0/300 (0%)]\tLoss: 0.401853\n",
      "Train Epoch: 99 [10/300 (3%)]\tLoss: 0.089167\n",
      "Train Epoch: 99 [20/300 (7%)]\tLoss: 0.180060\n",
      "Train Epoch: 99 [30/300 (10%)]\tLoss: 0.388708\n",
      "Train Epoch: 99 [40/300 (13%)]\tLoss: 0.301740\n",
      "Train Epoch: 99 [50/300 (17%)]\tLoss: 0.001839\n",
      "Train Epoch: 99 [60/300 (20%)]\tLoss: 0.064956\n",
      "Train Epoch: 99 [70/300 (23%)]\tLoss: 0.119321\n",
      "Train Epoch: 99 [80/300 (27%)]\tLoss: 0.127329\n",
      "Train Epoch: 99 [90/300 (30%)]\tLoss: 0.438142\n",
      "Train Epoch: 99 [100/300 (33%)]\tLoss: 0.740984\n",
      "Train Epoch: 99 [110/300 (37%)]\tLoss: 0.107059\n",
      "Train Epoch: 99 [120/300 (40%)]\tLoss: 0.150143\n",
      "Train Epoch: 99 [130/300 (43%)]\tLoss: 0.004414\n",
      "Train Epoch: 99 [140/300 (47%)]\tLoss: 0.043065\n",
      "Train Epoch: 99 [150/300 (50%)]\tLoss: 0.286225\n",
      "Train Epoch: 99 [160/300 (53%)]\tLoss: 0.149051\n",
      "Train Epoch: 99 [170/300 (57%)]\tLoss: 0.005432\n",
      "Train Epoch: 99 [180/300 (60%)]\tLoss: 0.511586\n",
      "Train Epoch: 99 [190/300 (63%)]\tLoss: 0.389245\n",
      "Train Epoch: 99 [200/300 (67%)]\tLoss: 0.238594\n",
      "Train Epoch: 99 [210/300 (70%)]\tLoss: 0.239985\n",
      "Train Epoch: 99 [220/300 (73%)]\tLoss: 0.115531\n",
      "Train Epoch: 99 [230/300 (77%)]\tLoss: 0.687191\n",
      "Train Epoch: 99 [240/300 (80%)]\tLoss: 0.521789\n",
      "Train Epoch: 99 [250/300 (83%)]\tLoss: 0.025897\n",
      "Train Epoch: 99 [260/300 (87%)]\tLoss: 0.241225\n",
      "Train Epoch: 99 [270/300 (90%)]\tLoss: 0.141635\n",
      "Train Epoch: 99 [280/300 (93%)]\tLoss: 0.003384\n",
      "Train Epoch: 99 [290/300 (97%)]\tLoss: 0.180791\n",
      "Train Epoch: 99 [0/300 (0%)]\tLoss: 0.109379\n",
      "Train Epoch: 99 [10/300 (3%)]\tLoss: 0.168830\n",
      "Train Epoch: 99 [20/300 (7%)]\tLoss: 0.159490\n",
      "Train Epoch: 99 [30/300 (10%)]\tLoss: 0.108214\n",
      "Train Epoch: 99 [40/300 (13%)]\tLoss: 0.438026\n",
      "Train Epoch: 99 [50/300 (17%)]\tLoss: 0.260052\n",
      "Train Epoch: 99 [60/300 (20%)]\tLoss: 0.794431\n",
      "Train Epoch: 99 [70/300 (23%)]\tLoss: 0.483774\n",
      "Train Epoch: 99 [80/300 (27%)]\tLoss: 0.084995\n",
      "Train Epoch: 99 [90/300 (30%)]\tLoss: 0.301317\n",
      "Train Epoch: 99 [100/300 (33%)]\tLoss: 0.573066\n",
      "Train Epoch: 99 [110/300 (37%)]\tLoss: 0.053481\n",
      "Train Epoch: 99 [120/300 (40%)]\tLoss: 0.245739\n",
      "Train Epoch: 99 [130/300 (43%)]\tLoss: 0.078977\n",
      "Train Epoch: 99 [140/300 (47%)]\tLoss: 0.324900\n",
      "Train Epoch: 99 [150/300 (50%)]\tLoss: 0.265703\n",
      "Train Epoch: 99 [160/300 (53%)]\tLoss: 0.410781\n",
      "Train Epoch: 99 [170/300 (57%)]\tLoss: 0.319265\n",
      "Train Epoch: 99 [180/300 (60%)]\tLoss: 0.090746\n",
      "Train Epoch: 99 [190/300 (63%)]\tLoss: 0.561360\n",
      "Train Epoch: 99 [200/300 (67%)]\tLoss: 0.015465\n",
      "Train Epoch: 99 [210/300 (70%)]\tLoss: 0.028511\n",
      "Train Epoch: 99 [220/300 (73%)]\tLoss: 0.405266\n",
      "Train Epoch: 99 [230/300 (77%)]\tLoss: 0.056012\n",
      "Train Epoch: 99 [240/300 (80%)]\tLoss: 0.022679\n",
      "Train Epoch: 99 [250/300 (83%)]\tLoss: 0.307096\n",
      "Train Epoch: 99 [260/300 (87%)]\tLoss: 0.567691\n",
      "Train Epoch: 99 [270/300 (90%)]\tLoss: 0.220555\n",
      "Train Epoch: 99 [280/300 (93%)]\tLoss: 0.192252\n",
      "Train Epoch: 99 [290/300 (97%)]\tLoss: 0.034584\n",
      "Train Epoch: 99 [0/300 (0%)]\tLoss: 0.315518\n",
      "Train Epoch: 99 [10/300 (3%)]\tLoss: 0.026794\n",
      "Train Epoch: 99 [20/300 (7%)]\tLoss: 0.033985\n",
      "Train Epoch: 99 [30/300 (10%)]\tLoss: 0.339822\n",
      "Train Epoch: 99 [40/300 (13%)]\tLoss: 1.305575\n",
      "Train Epoch: 99 [50/300 (17%)]\tLoss: 0.179431\n",
      "Train Epoch: 99 [60/300 (20%)]\tLoss: 0.268810\n",
      "Train Epoch: 99 [70/300 (23%)]\tLoss: 0.511195\n",
      "Train Epoch: 99 [80/300 (27%)]\tLoss: 0.071710\n",
      "Train Epoch: 99 [90/300 (30%)]\tLoss: 0.077772\n",
      "Train Epoch: 99 [100/300 (33%)]\tLoss: 0.642867\n",
      "Train Epoch: 99 [110/300 (37%)]\tLoss: 0.101059\n",
      "Train Epoch: 99 [120/300 (40%)]\tLoss: 0.630770\n",
      "Train Epoch: 99 [130/300 (43%)]\tLoss: 1.034653\n",
      "Train Epoch: 99 [140/300 (47%)]\tLoss: 0.295020\n",
      "Train Epoch: 99 [150/300 (50%)]\tLoss: 0.741871\n",
      "Train Epoch: 99 [160/300 (53%)]\tLoss: 0.305280\n",
      "Train Epoch: 99 [170/300 (57%)]\tLoss: 0.354660\n",
      "Train Epoch: 99 [180/300 (60%)]\tLoss: 0.037247\n",
      "Train Epoch: 99 [190/300 (63%)]\tLoss: 0.477176\n",
      "Train Epoch: 99 [200/300 (67%)]\tLoss: 0.803475\n",
      "Train Epoch: 99 [210/300 (70%)]\tLoss: 0.248562\n",
      "Train Epoch: 99 [220/300 (73%)]\tLoss: 2.111447\n",
      "Train Epoch: 99 [230/300 (77%)]\tLoss: 0.347229\n",
      "Train Epoch: 99 [240/300 (80%)]\tLoss: 0.716010\n",
      "Train Epoch: 99 [250/300 (83%)]\tLoss: 0.423224\n",
      "Train Epoch: 99 [260/300 (87%)]\tLoss: 0.325553\n",
      "Train Epoch: 99 [270/300 (90%)]\tLoss: 0.460947\n",
      "Train Epoch: 99 [280/300 (93%)]\tLoss: 0.029155\n",
      "Train Epoch: 99 [290/300 (97%)]\tLoss: 0.461056\n",
      "Model aggregation in round 99 was successful\n",
      "Model aggregation in round 149 was successful\n",
      "Train Epoch: 199 [0/300 (0%)]\tLoss: 0.103294\n",
      "Train Epoch: 199 [10/300 (3%)]\tLoss: 0.665208\n",
      "Train Epoch: 199 [20/300 (7%)]\tLoss: 0.339948\n",
      "Train Epoch: 199 [30/300 (10%)]\tLoss: 0.358490\n",
      "Train Epoch: 199 [40/300 (13%)]\tLoss: 0.219474\n",
      "Train Epoch: 199 [50/300 (17%)]\tLoss: 0.403988\n",
      "Train Epoch: 199 [60/300 (20%)]\tLoss: 0.537907\n",
      "Train Epoch: 199 [70/300 (23%)]\tLoss: 0.344837\n",
      "Train Epoch: 199 [80/300 (27%)]\tLoss: 0.301032\n",
      "Train Epoch: 199 [90/300 (30%)]\tLoss: 0.519411\n",
      "Train Epoch: 199 [100/300 (33%)]\tLoss: 0.300249\n",
      "Train Epoch: 199 [110/300 (37%)]\tLoss: 0.151904\n",
      "Train Epoch: 199 [120/300 (40%)]\tLoss: 0.769145\n",
      "Train Epoch: 199 [130/300 (43%)]\tLoss: 0.014975\n",
      "Train Epoch: 199 [140/300 (47%)]\tLoss: 0.130806\n",
      "Train Epoch: 199 [150/300 (50%)]\tLoss: 0.480281\n",
      "Train Epoch: 199 [160/300 (53%)]\tLoss: 0.240300\n",
      "Train Epoch: 199 [170/300 (57%)]\tLoss: 0.167606\n",
      "Train Epoch: 199 [180/300 (60%)]\tLoss: 0.288791\n",
      "Train Epoch: 199 [190/300 (63%)]\tLoss: 0.040569\n",
      "Train Epoch: 199 [200/300 (67%)]\tLoss: 0.110156\n",
      "Train Epoch: 199 [210/300 (70%)]\tLoss: 0.045933\n",
      "Train Epoch: 199 [220/300 (73%)]\tLoss: 0.317567\n",
      "Train Epoch: 199 [230/300 (77%)]\tLoss: 0.453993\n",
      "Train Epoch: 199 [240/300 (80%)]\tLoss: 0.103948\n",
      "Train Epoch: 199 [250/300 (83%)]\tLoss: 0.458277\n",
      "Train Epoch: 199 [260/300 (87%)]\tLoss: 0.465812\n",
      "Train Epoch: 199 [270/300 (90%)]\tLoss: 0.011357\n",
      "Train Epoch: 199 [280/300 (93%)]\tLoss: 0.234086\n",
      "Train Epoch: 199 [290/300 (97%)]\tLoss: 0.032978\n",
      "Train Epoch: 199 [0/300 (0%)]\tLoss: 0.074857\n",
      "Train Epoch: 199 [10/300 (3%)]\tLoss: 0.239087\n",
      "Train Epoch: 199 [20/300 (7%)]\tLoss: 0.140576\n",
      "Train Epoch: 199 [30/300 (10%)]\tLoss: 0.490130\n",
      "Train Epoch: 199 [40/300 (13%)]\tLoss: 1.377469\n",
      "Train Epoch: 199 [50/300 (17%)]\tLoss: 0.057245\n",
      "Train Epoch: 199 [60/300 (20%)]\tLoss: 0.176287\n",
      "Train Epoch: 199 [70/300 (23%)]\tLoss: 0.080333\n",
      "Train Epoch: 199 [80/300 (27%)]\tLoss: 0.710710\n",
      "Train Epoch: 199 [90/300 (30%)]\tLoss: 0.050278\n",
      "Train Epoch: 199 [100/300 (33%)]\tLoss: 0.101889\n",
      "Train Epoch: 199 [110/300 (37%)]\tLoss: 0.072366\n",
      "Train Epoch: 199 [120/300 (40%)]\tLoss: 0.964246\n",
      "Train Epoch: 199 [130/300 (43%)]\tLoss: 0.107090\n",
      "Train Epoch: 199 [140/300 (47%)]\tLoss: 0.649594\n",
      "Train Epoch: 199 [150/300 (50%)]\tLoss: 0.401168\n",
      "Train Epoch: 199 [160/300 (53%)]\tLoss: 0.468983\n",
      "Train Epoch: 199 [170/300 (57%)]\tLoss: 0.230528\n",
      "Train Epoch: 199 [180/300 (60%)]\tLoss: 0.047584\n",
      "Train Epoch: 199 [190/300 (63%)]\tLoss: 0.014042\n",
      "Train Epoch: 199 [200/300 (67%)]\tLoss: 0.469855\n",
      "Train Epoch: 199 [210/300 (70%)]\tLoss: 0.263362\n",
      "Train Epoch: 199 [220/300 (73%)]\tLoss: 0.618360\n",
      "Train Epoch: 199 [230/300 (77%)]\tLoss: 1.742619\n",
      "Train Epoch: 199 [240/300 (80%)]\tLoss: 0.828481\n",
      "Train Epoch: 199 [250/300 (83%)]\tLoss: 0.642565\n",
      "Train Epoch: 199 [260/300 (87%)]\tLoss: 0.785518\n",
      "Train Epoch: 199 [270/300 (90%)]\tLoss: 0.147512\n",
      "Train Epoch: 199 [280/300 (93%)]\tLoss: 0.054094\n",
      "Train Epoch: 199 [290/300 (97%)]\tLoss: 0.390944\n",
      "Train Epoch: 199 [0/300 (0%)]\tLoss: 0.224492\n",
      "Train Epoch: 199 [10/300 (3%)]\tLoss: 0.112501\n",
      "Train Epoch: 199 [20/300 (7%)]\tLoss: 0.049418\n",
      "Train Epoch: 199 [30/300 (10%)]\tLoss: 0.519234\n",
      "Train Epoch: 199 [40/300 (13%)]\tLoss: 0.214607\n",
      "Train Epoch: 199 [50/300 (17%)]\tLoss: 0.334566\n",
      "Train Epoch: 199 [60/300 (20%)]\tLoss: 0.146946\n",
      "Train Epoch: 199 [70/300 (23%)]\tLoss: 0.797781\n",
      "Train Epoch: 199 [80/300 (27%)]\tLoss: 0.017531\n",
      "Train Epoch: 199 [90/300 (30%)]\tLoss: 0.577533\n",
      "Train Epoch: 199 [100/300 (33%)]\tLoss: 0.057115\n",
      "Train Epoch: 199 [110/300 (37%)]\tLoss: 0.213822\n",
      "Train Epoch: 199 [120/300 (40%)]\tLoss: 0.366782\n",
      "Train Epoch: 199 [130/300 (43%)]\tLoss: 0.251706\n",
      "Train Epoch: 199 [140/300 (47%)]\tLoss: 0.089293\n",
      "Train Epoch: 199 [150/300 (50%)]\tLoss: 0.103488\n",
      "Train Epoch: 199 [160/300 (53%)]\tLoss: 0.034905\n",
      "Train Epoch: 199 [170/300 (57%)]\tLoss: 0.053242\n",
      "Train Epoch: 199 [180/300 (60%)]\tLoss: 0.632892\n",
      "Train Epoch: 199 [190/300 (63%)]\tLoss: 0.028202\n",
      "Train Epoch: 199 [200/300 (67%)]\tLoss: 0.132956\n",
      "Train Epoch: 199 [210/300 (70%)]\tLoss: 0.078076\n",
      "Train Epoch: 199 [220/300 (73%)]\tLoss: 0.003448\n",
      "Train Epoch: 199 [230/300 (77%)]\tLoss: 0.098639\n",
      "Train Epoch: 199 [240/300 (80%)]\tLoss: 0.823282\n",
      "Train Epoch: 199 [250/300 (83%)]\tLoss: 0.171507\n",
      "Train Epoch: 199 [260/300 (87%)]\tLoss: 0.014101\n",
      "Train Epoch: 199 [270/300 (90%)]\tLoss: 0.459960\n",
      "Train Epoch: 199 [280/300 (93%)]\tLoss: 0.573647\n",
      "Train Epoch: 199 [290/300 (97%)]\tLoss: 0.063310\n",
      "Train Epoch: 199 [0/300 (0%)]\tLoss: 0.001567\n",
      "Train Epoch: 199 [10/300 (3%)]\tLoss: 0.011276\n",
      "Train Epoch: 199 [20/300 (7%)]\tLoss: 0.158499\n",
      "Train Epoch: 199 [30/300 (10%)]\tLoss: 0.012417\n",
      "Train Epoch: 199 [40/300 (13%)]\tLoss: 0.006314\n",
      "Train Epoch: 199 [50/300 (17%)]\tLoss: 0.004749\n",
      "Train Epoch: 199 [60/300 (20%)]\tLoss: 0.028164\n",
      "Train Epoch: 199 [70/300 (23%)]\tLoss: 0.420102\n",
      "Train Epoch: 199 [80/300 (27%)]\tLoss: 0.166740\n",
      "Train Epoch: 199 [90/300 (30%)]\tLoss: 0.300766\n",
      "Train Epoch: 199 [100/300 (33%)]\tLoss: 0.012121\n",
      "Train Epoch: 199 [110/300 (37%)]\tLoss: 0.034744\n",
      "Train Epoch: 199 [120/300 (40%)]\tLoss: 0.149138\n",
      "Train Epoch: 199 [130/300 (43%)]\tLoss: 0.180751\n",
      "Train Epoch: 199 [140/300 (47%)]\tLoss: 0.063046\n",
      "Train Epoch: 199 [150/300 (50%)]\tLoss: 0.084906\n",
      "Train Epoch: 199 [160/300 (53%)]\tLoss: 0.255354\n",
      "Train Epoch: 199 [170/300 (57%)]\tLoss: 0.044541\n",
      "Train Epoch: 199 [180/300 (60%)]\tLoss: 0.043747\n",
      "Train Epoch: 199 [190/300 (63%)]\tLoss: 0.029240\n",
      "Train Epoch: 199 [200/300 (67%)]\tLoss: 0.026758\n",
      "Train Epoch: 199 [210/300 (70%)]\tLoss: 1.131502\n",
      "Train Epoch: 199 [220/300 (73%)]\tLoss: 0.076107\n",
      "Train Epoch: 199 [230/300 (77%)]\tLoss: 0.005573\n",
      "Train Epoch: 199 [240/300 (80%)]\tLoss: 0.073126\n",
      "Train Epoch: 199 [250/300 (83%)]\tLoss: 0.163135\n",
      "Train Epoch: 199 [260/300 (87%)]\tLoss: 0.047534\n",
      "Train Epoch: 199 [270/300 (90%)]\tLoss: 0.067858\n",
      "Train Epoch: 199 [280/300 (93%)]\tLoss: 0.131615\n",
      "Train Epoch: 199 [290/300 (97%)]\tLoss: 0.016315\n",
      "Train Epoch: 199 [0/300 (0%)]\tLoss: 0.084080\n",
      "Train Epoch: 199 [10/300 (3%)]\tLoss: 0.309589\n",
      "Train Epoch: 199 [20/300 (7%)]\tLoss: 0.078284\n",
      "Train Epoch: 199 [30/300 (10%)]\tLoss: 0.790364\n",
      "Train Epoch: 199 [40/300 (13%)]\tLoss: 0.004847\n",
      "Train Epoch: 199 [50/300 (17%)]\tLoss: 0.210713\n",
      "Train Epoch: 199 [60/300 (20%)]\tLoss: 0.139571\n",
      "Train Epoch: 199 [70/300 (23%)]\tLoss: 0.024331\n",
      "Train Epoch: 199 [80/300 (27%)]\tLoss: 0.040116\n",
      "Train Epoch: 199 [90/300 (30%)]\tLoss: 0.121432\n",
      "Train Epoch: 199 [100/300 (33%)]\tLoss: 0.140238\n",
      "Train Epoch: 199 [110/300 (37%)]\tLoss: 0.608360\n",
      "Train Epoch: 199 [120/300 (40%)]\tLoss: 0.546878\n",
      "Train Epoch: 199 [130/300 (43%)]\tLoss: 0.266896\n",
      "Train Epoch: 199 [140/300 (47%)]\tLoss: 0.196613\n",
      "Train Epoch: 199 [150/300 (50%)]\tLoss: 0.124564\n",
      "Train Epoch: 199 [160/300 (53%)]\tLoss: 0.498816\n",
      "Train Epoch: 199 [170/300 (57%)]\tLoss: 0.334873\n",
      "Train Epoch: 199 [180/300 (60%)]\tLoss: 0.065983\n",
      "Train Epoch: 199 [190/300 (63%)]\tLoss: 0.065774\n",
      "Train Epoch: 199 [200/300 (67%)]\tLoss: 0.108664\n",
      "Train Epoch: 199 [210/300 (70%)]\tLoss: 0.313470\n",
      "Train Epoch: 199 [220/300 (73%)]\tLoss: 0.974251\n",
      "Train Epoch: 199 [230/300 (77%)]\tLoss: 0.547204\n",
      "Train Epoch: 199 [240/300 (80%)]\tLoss: 0.196929\n",
      "Train Epoch: 199 [250/300 (83%)]\tLoss: 0.216671\n",
      "Train Epoch: 199 [260/300 (87%)]\tLoss: 0.065978\n",
      "Train Epoch: 199 [270/300 (90%)]\tLoss: 0.444126\n",
      "Train Epoch: 199 [280/300 (93%)]\tLoss: 0.038699\n",
      "Train Epoch: 199 [290/300 (97%)]\tLoss: 0.011282\n",
      "Model aggregation in round 199 was successful\n"
     ]
    }
   ],
   "source": [
    "server = Server(config, shap_util)\n",
    "client_plane = ClientPlane(config, observer_config, data, shap_util)\n",
    "\n",
    "def run_round():\n",
    "    client_plane.update_clients(server.get_nn_parameters())\n",
    "    selected_clients = server.select_clients()\n",
    "    client_parameters = client_plane.train_selected_clients(selected_clients)\n",
    "    server.aggregate_model(client_parameters)\n",
    "    \n",
    "for i in range(200):\n",
    "    client_plane.set_rounds(i)\n",
    "    server.set_rounds(i)\n",
    "    run_round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9d2930-6e4b-4438-b8ef-0b3366899843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "server.get_shap_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fec40-a8b3-4726-9905-dc7702e2a6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
