{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91c5a86-2173-4938-b072-60b6b688e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_learning.dataloader import MnistDataloader\n",
    "from federated_learning.configuration import Configuration\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f3037c-68f9-4a6d-af05-97b8aba74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6f3d4e-5db9-4b39-80ee-23b0429a2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f39002e4fb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration \n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6d65b2-1ccf-444b-8bd8-3646431804cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training loader loaded.\n",
      "MNIST test loader loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Configuration()\n",
    "mnist = MNISTDataloader(config)\n",
    "examples = enumerate(mnist.train_dataloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbc5dde-946d-4eb7-912c-f795032455f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0095f-268d-4547-a439-0db4b9dd722c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcea48-257a-4b31-8f64-85c8f56fa051",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93547285-24f5-4945-91da-19242a7e1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c8f6f4-dcd0-4ecf-acf3-33d05518a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310f1903-285c-4320-ad7c-efb84f65649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10292430-56cb-4518-9d0e-022c75e60d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(mnist.train_dataloader.dataset) for i in range(n_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d179f4d-4d8e-4c25-8437-591f9773fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Funktions\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(mnist.train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(mnist.train_dataloader.dataset),100. * batch_idx / len(mnist.train_dataloader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(mnist.train_dataloader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in mnist.test_dataloader:\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            total += 1\n",
    "            print(test_loss, loss, total)\n",
    "    test_loss /= total\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(mnist.test_dataloader.dataset), 100. * correct / len(mnist.test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd299bbf-63fd-40af-a08e-42e931300747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9119/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14098504185676575 tensor(0.1410) 1\n",
      "0.3577156513929367 tensor(0.2167) 2\n",
      "0.5330999791622162 tensor(0.1754) 3\n",
      "0.7203625738620758 tensor(0.1873) 4\n",
      "0.9067732691764832 tensor(0.1864) 5\n",
      "0.996419832110405 tensor(0.0896) 6\n",
      "1.1256583631038666 tensor(0.1292) 7\n",
      "1.227224551141262 tensor(0.1016) 8\n",
      "1.2783867716789246 tensor(0.0512) 9\n",
      "1.4146123975515366 tensor(0.1362) 10\n",
      "\n",
      "Test set: Avg. loss: 0.1415, Accuracy: 9559/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.309653\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.293267\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.300590\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.237809\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.248873\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.185987\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.240333\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.324401\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.495263\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.464126\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.322244\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.308772\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.383400\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.312980\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.212213\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.420716\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.592553\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.331909\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.649265\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.234354\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.249052\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.190619\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.416564\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.658286\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.259407\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.491750\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.438581\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.197217\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.147638\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.211216\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.391650\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.362968\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.265604\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.464433\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.173340\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.305651\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.427001\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.380655\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.188770\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.325120\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.288319\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.413294\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.373399\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.266275\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.365316\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.483164\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.340405\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.357592\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.277499\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.399374\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.427017\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.306806\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.269446\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.194205\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.316823\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.593317\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.401715\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.333063\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.257574\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.404429\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.242120\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.146007\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.333162\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.160477\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.378274\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.295443\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.298511\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.431070\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.310806\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.279965\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.349347\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.476243\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.150004\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.351133\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.302214\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.306955\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.361482\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.185616\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.310397\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.289041\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.282462\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.299433\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.114790\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.351831\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.253497\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.157869\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.355425\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.379477\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.359684\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.380035\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.463033\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.172445\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.090903\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.150883\n",
      "0.1314486265182495 tensor(0.1314) 1\n",
      "0.3282843828201294 tensor(0.1968) 2\n",
      "0.49355632066726685 tensor(0.1653) 3\n",
      "0.6699131727218628 tensor(0.1764) 4\n",
      "0.8503328859806061 tensor(0.1804) 5\n",
      "0.9336463436484337 tensor(0.0833) 6\n",
      "1.0290717408061028 tensor(0.0954) 7\n",
      "1.0941299349069595 tensor(0.0651) 8\n",
      "1.1277430579066277 tensor(0.0336) 9\n",
      "1.237116500735283 tensor(0.1094) 10\n",
      "\n",
      "Test set: Avg. loss: 0.1237, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.239838\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.200691\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.306529\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.338205\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.353177\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.348425\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.202766\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.229629\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.539918\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.257281\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.323980\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.212082\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.300469\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.222237\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.208525\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.361720\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.659827\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.367563\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.576183\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.386455\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.178061\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.199019\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.315959\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.616827\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.378329\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.450051\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.464734\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.348021\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.275318\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.293309\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.388356\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.214412\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.137954\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.397762\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.189503\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.257956\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.426420\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.711431\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.099865\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.172721\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.227016\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.306947\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.291175\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.346721\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.361170\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.278333\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.237422\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.309803\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.309348\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.407009\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.315061\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.272279\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.252126\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.219595\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.250333\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.440913\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.342525\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.268706\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.276074\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.380584\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.257783\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.060986\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.256404\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.229373\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.408860\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.396444\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.251593\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.458406\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.426123\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.196739\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.526021\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.541384\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.311127\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.315004\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.373847\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.311032\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.331683\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.248186\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.252376\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.252212\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.400926\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.166758\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.139979\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.348825\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.272533\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.238847\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.171576\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.413312\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.389493\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.186167\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.445890\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.156411\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.088157\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.124884\n",
      "0.10680553317070007 tensor(0.1068) 1\n",
      "0.2773497402667999 tensor(0.1705) 2\n",
      "0.41850730776786804 tensor(0.1412) 3\n",
      "0.5770728290081024 tensor(0.1586) 4\n",
      "0.7311952114105225 tensor(0.1541) 5\n",
      "0.790226012468338 tensor(0.0590) 6\n",
      "0.8663552403450012 tensor(0.0761) 7\n",
      "0.9203099608421326 tensor(0.0540) 8\n",
      "0.9475733377039433 tensor(0.0273) 9\n",
      "1.0433538816869259 tensor(0.0958) 10\n",
      "\n",
      "Test set: Avg. loss: 0.1043, Accuracy: 9665/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.308738\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.183157\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.225164\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.299102\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.253657\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.391343\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.158638\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.177795\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.487628\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.308257\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.321708\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.247023\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.270291\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.220931\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.272290\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.281876\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.520774\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.263627\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.636474\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.359573\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.195284\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.237220\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.312585\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.632171\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.311707\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.371656\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.353463\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.146372\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.203318\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.322030\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.298772\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.205645\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.217147\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.287809\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.179258\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.253800\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.602862\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.759864\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.164884\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.306482\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.337188\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.201527\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.319730\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.262355\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.238531\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.366585\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.364682\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.264829\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.255525\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.266350\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.303457\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.244346\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.125456\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.150656\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.328199\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.338847\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.274445\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.180107\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.312666\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.278494\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.210035\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.119113\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.270599\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.190816\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.285597\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.260483\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.256485\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.399520\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.252979\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.240340\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.335528\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.408069\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.422638\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.364111\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.352194\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.241144\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.249960\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.135051\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.278640\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.296109\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.287224\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.235493\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.103223\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.287821\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.117670\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.233224\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.231795\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.198510\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.288506\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.267834\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.386449\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.105529\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.143921\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.102716\n",
      "0.09507929533720016 tensor(0.0951) 1\n",
      "0.25586163252592087 tensor(0.1608) 2\n",
      "0.38163813203573227 tensor(0.1258) 3\n",
      "0.5333421155810356 tensor(0.1517) 4\n",
      "0.6799052283167839 tensor(0.1466) 5\n",
      "0.7323769703507423 tensor(0.0525) 6\n",
      "0.8041422888636589 tensor(0.0718) 7\n",
      "0.8503334634006023 tensor(0.0462) 8\n",
      "0.8711897470057011 tensor(0.0209) 9\n",
      "0.956857081502676 tensor(0.0857) 10\n",
      "\n",
      "Test set: Avg. loss: 0.0957, Accuracy: 9686/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
