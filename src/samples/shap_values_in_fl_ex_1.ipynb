{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298dfa70-a46d-4329-b1e5-b7944d8ade29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering with SHAP values Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadba9b-4644-4998-bdeb-39dee05965cc",
   "metadata": {},
   "source": [
    "SHAP Images of server with different number of poisoned clients\n",
    "* number of malicious clients [0,1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa09c19-b79a-4d04-91f1-d27bd6be9260",
   "metadata": {},
   "source": [
    "SHAP Images right after poisoning attack\n",
    "* number of malicious clients [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f5fea-12fa-4413-be81-1126656be438",
   "metadata": {},
   "source": [
    "Summation of Differences SHAP Images right after poisoning attack\n",
    "* rounds [1,2,10,75,200]\n",
    "* 5 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccac5ea-7b1d-47c8-8a4e-c3d18bfe5c81",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265651c-c72f-43f7-b5e9-a3ea624b981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.flush_and_unmount()\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746fc0b-bb90-4b51-ac7d-fc778b8a9438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks')\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/federated_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d30812-8784-458b-9f75-c3b9dccff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap==0.40.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3550e57-ade5-4232-86c0-4030585df070",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652af317-4a22-4efc-adda-a19cb17141dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_learning.utils import SHAPUtil, experiment_util, Visualizer\n",
    "from federated_learning import ClientPlane, Configuration, ObserverConfiguration\n",
    "from federated_learning.server import Server\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5a68f-c527-4aef-86b3-0f8ef24ec205",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MNIST\n",
    "(1) 5 → 4,\n",
    "(2) 1 → 7,\n",
    "(3) 3 → 8,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263060ac-7edc-4e96-8a27-5cea91b7618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_learning.nets import MNISTCNN\n",
    "from federated_learning.dataset import MNISTDataset\n",
    "import os\n",
    "config = Configuration()\n",
    "config.POISONED_CLIENTS = 0\n",
    "config.DATA_POISONING_PERCENTAGE = 1\n",
    "config.DATASET = MNISTDataset\n",
    "config.MODELNAME = config.MNIST_NAME\n",
    "config.NETWORK = MNISTCNN\n",
    "observer_config = ObserverConfiguration()\n",
    "observer_config.experiment_type = \"shap_fl_poisoned\"\n",
    "observer_config.experiment_id = 1\n",
    "observer_config.test = False\n",
    "observer_config.datasetObserverConfiguration = \"MNIST\"\n",
    "neutral_label = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c85810-ebed-4b4e-a093-5fd61d41f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Settigns\n",
    "config.TEMP = os.path.join('/content/drive/My Drive/Colab Notebooks/temp')\n",
    "config.FMNIST_DATASET_PATH = os.path.join('/content/data/fmnist')\n",
    "config.MNIST_DATASET_PATH = os.path.join('/content/data/mnist')\n",
    "config.CIFAR10_DATASET_PATH = os.path.join('/content/data/cifar10')\n",
    "config.VM_URL = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8e9163-199f-4570-9e62-06c76a1099c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training data loaded.\n",
      "MNIST test data loaded.\n",
      "Create 200 clients with dataset of size 300\n"
     ]
    }
   ],
   "source": [
    "data = config.DATASET(config)\n",
    "shap_util = SHAPUtil(data.test_dataloader) \n",
    "server = Server(config, observer_config,data.train_dataloader, data.test_dataloader, shap_util)\n",
    "client_plane = ClientPlane(config, observer_config, data, shap_util)\n",
    "visualizer = Visualizer(shap_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed17f23-8466-4113-9aac-1f1f2d467afa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "for i in range(199):\n",
    "    experiment_util.set_rounds(client_plane, server, i+1)\n",
    "    experiment_util.run_round(client_plane, server, i+1)\n",
    "print(\"Run 199 finished\")\n",
    "old_params = copy.deepcopy(server.get_nn_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa35924-ee48-42ce-95aa-ee77a851bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import torch\n",
    "#torch.save(server.net.state_dict(), \"temp/models/MNISTtrained2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e51796-b060-459a-a750-f2fc8c673fc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### alpha (5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c1f0e-8a6c-4c66-81b8-53b4988c8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.FROM_LABEL = 5\n",
    "config.TO_LABEL = 4\n",
    "\n",
    "accuracies = []\n",
    "recalls = []\n",
    "\n",
    "import torch\n",
    "torch.save(server.net.state_dict(), \"temp/models/MNISTtrained.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d8664-1a63-45c1-a86f-d7c71e684370",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(config.CLIENTS_PER_ROUND + 1):\n",
    "    server.update_nn_parameters(old_params)\n",
    "    server.test()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    print(\"Original\", recall, precision, accuracy)\n",
    "    file_name = './results/ex3/MNIST/5_4/_run_shap_values_{}_poisoned_clients_alpha_5_4.pdf'.format(j)\n",
    "    config.POISONED_CLIENTS = j\n",
    "    experiment_util.update_configs(client_plane, server, config, observer_config)\n",
    "    client_plane.poison_clients()\n",
    "    clean_clients = experiment_util.select_random_clean(client_plane, config, config.CLIENTS_PER_ROUND - j)\n",
    "    poisoned_clients = experiment_util.select_poisoned(client_plane, j)\n",
    "    clients = [*clean_clients, *poisoned_clients]\n",
    "    print(clients)\n",
    "    experiment_util.run_round_with(clients, old_params, client_plane, server, 200)\n",
    "    server.test()\n",
    "    server_shap = server.get_shap_values()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    visualizer.plot_shap_values(server_shap,file_name)\n",
    "    print(recall, precision, accuracy)\n",
    "    print(\"Poisoned clients: {}\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c951659-99b5-4bf1-a915-ddf91539e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 0\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70cccf-6929-4a0f-b1b6-4db2b04ace36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1\n",
    "print(accuracies[6:])\n",
    "print(recalls[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b85f7-5b3a-4666-b80a-f104fcee1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2\n",
    "print(accuracies[12:])\n",
    "print(recalls[12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475413a4-4b12-4cea-82f2-d9ae34e15663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e50740-62b8-431a-aec4-06be737f828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fed49-30dc-4953-b63f-f5627d9bfafe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### alpha (1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e9c720-bc7a-425a-82ad-9bc3d44455f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from federated_learning.nets import MNISTCNN\n",
    "config.FROM_LABEL = 1\n",
    "config.TO_LABEL = 7\n",
    "\n",
    "server.net =  MNISTCNN()\n",
    "server.net.load_state_dict(torch.load('temp/models/MNISTtrained2.model'))\n",
    "old_params = copy.deepcopy(server.get_nn_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b3328-8cc6-4f3d-ae03-c823f5a7a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b92fd-1b96-4596-bc9a-b746e0f4ddb2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(config.CLIENTS_PER_ROUND + 1):\n",
    "    server.update_nn_parameters(old_params)\n",
    "    server.test()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    print(\"Original\", recall, precision, accuracy)\n",
    "    file_name = './results/ex3/MNIST/1_7/4_run_shap_values_{}_poisoned_clients_alpha_1_7.pdf'.format(j)\n",
    "    config.POISONED_CLIENTS = j\n",
    "    experiment_util.update_configs(client_plane, server, config, observer_config)\n",
    "    client_plane.poison_clients()\n",
    "    clean_clients = experiment_util.select_random_clean(client_plane, config, config.CLIENTS_PER_ROUND - j)\n",
    "    poisoned_clients = experiment_util.select_poisoned(client_plane, j)\n",
    "    clients = [*clean_clients, *poisoned_clients]\n",
    "    print(clients)\n",
    "    experiment_util.run_round_with(clients, old_params, client_plane, server, 200)\n",
    "    server.test()\n",
    "    server_shap = server.get_shap_values()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    visualizer.plot_shap_values(server_shap,file_name)\n",
    "    print(recall, precision, accuracy)\n",
    "    print(\"Poisoned clients: {}\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ecd9a-4249-4dbf-a115-54b2f320fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 0\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477daf9-52b9-490d-b0a7-74b849a0f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1\n",
    "print(accuracies[6:])\n",
    "print(recalls[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2735af4-a18d-4bd5-8ede-b54c1ba4f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447830e-a036-43a5-a63a-d8f3a4785fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3\n",
    "print(accuracies[6:])\n",
    "print(recalls[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb27cec-5749-440a-a660-6ded56aad068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de1ca5-accc-426b-ba3d-4db2f72c388d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### alpha (3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3001db44-38e1-4185-81db-718ce2606f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from federated_learning.nets import MNISTCNN\n",
    "config.FROM_LABEL = 3\n",
    "config.TO_LABEL = 8\n",
    "\n",
    "server.net =  MNISTCNN()\n",
    "server.net.load_state_dict(torch.load('temp/models/MNISTtrained2.model'))\n",
    "old_params = copy.deepcopy(server.get_nn_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f60fe6-04eb-4f3d-8108-c10448e514d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7aa466-5bc3-4c0d-a43d-67bd47310079",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Original tensor([0.9929, 0.9921, 0.9787, 0.9832, 0.9745, 0.9709, 0.9823, 0.9582, 0.9548,\n",
      "        0.9594]) tensor([0.9653, 0.9869, 0.9555, 0.9585, 0.9846, 0.9807, 0.9864, 0.9733, 0.9862,\n",
      "        0.9738]) 0.9749\n",
      "No poisoning due to 0. poisoned clients\n",
      "[110, 22, 170, 185, 52]\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.107917\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.092170\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.223941\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.612377\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.029483\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.023221\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.111617\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.040513\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.454605\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.073538\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.292561\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.009152\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.278231\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.121038\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.539021\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.002969\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.191753\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.179778\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.411535\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.675357\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.008562\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.510366\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.016477\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.151627\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.053579\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.257373\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.362258\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.056688\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.191796\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.220399\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.057525\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 3.135629\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 2.333744\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.308128\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.410968\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.095822\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.295158\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.583153\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.962004\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.438749\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.245579\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.002736\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.197679\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.275525\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.085587\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.107387\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.059895\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.309927\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.043431\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.505646\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.210229\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.379477\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.598948\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.947185\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.026867\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.882263\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.352502\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.452238\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.520231\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.075002\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.038935\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.704939\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 2.275823\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 1.140164\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 1.371636\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.461761\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 1.333591\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.047887\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.096876\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.193090\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.492208\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.240258\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.237913\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.102540\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.114760\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.481710\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.159500\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.385504\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.433020\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.512085\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.329324\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.301891\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.170953\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.123273\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.094030\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.150807\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.209017\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.342106\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.313627\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.234001\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.067169\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.055023\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.003044\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.266588\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.086004\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.367036\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.079976\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.100260\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.025763\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.328205\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.387182\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.160974\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.003706\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.039514\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.090095\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.002769\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.001075\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.037300\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.461505\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.008288\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.116888\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.568832\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.363469\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.244326\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.004220\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.028644\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.128641\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.039986\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.050157\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.125665\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.274828\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.202394\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.062091\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.058670\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 1.057038\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.201541\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.035893\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.834430\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.117949\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.029265\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.002533\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.077845\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.068609\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.047740\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.566906\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.023306\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.155194\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.310408\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.209676\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.781505\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.105322\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.238064\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.256739\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.003438\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.252892\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.269775\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.190515\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.161785\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.233113\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.795643\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9721/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make picture\n",
      "Make picture\n",
      "tensor([0.9929, 0.9903, 0.9816, 0.9168, 0.9796, 0.9798, 0.9833, 0.9611, 0.9702,\n",
      "        0.9653]) tensor([0.9701, 0.9877, 0.9602, 0.9957, 0.9816, 0.9552, 0.9812, 0.9734, 0.9459,\n",
      "        0.9692]) 0.9721\n",
      "Poisoned clients: 0\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Original tensor([0.9929, 0.9921, 0.9787, 0.9832, 0.9745, 0.9709, 0.9823, 0.9582, 0.9548,\n",
      "        0.9594]) tensor([0.9653, 0.9869, 0.9555, 0.9585, 0.9846, 0.9807, 0.9864, 0.9733, 0.9862,\n",
      "        0.9738]) 0.9749\n",
      "Poison 1/200 clients\n",
      "Flip 100.0% of the 3 labels to 8\n",
      "[134]\n",
      "[72, 93, 151, 79, 134]\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.037225\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.009104\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.909985\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.759963\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.032215\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 1.172093\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.643163\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.483580\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.361590\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.210046\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.476571\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.192094\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.342510\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.055370\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.628379\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.437294\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.255873\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.228685\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.697949\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.533670\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.371294\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.117782\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.205704\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.263312\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.742412\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.056703\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.451104\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.395742\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.233998\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.291701\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.317873\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.327154\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.696692\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.185562\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.058135\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.094054\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.032568\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.007848\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.381289\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.050833\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.166928\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.028502\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.007406\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.067185\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.144792\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.063041\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.128503\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.005227\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.008277\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.109870\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.176893\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.011194\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.011741\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.041102\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.016976\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.003974\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.064006\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.115302\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.732471\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.034883\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.064863\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.006395\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.272800\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.066905\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.009997\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.016482\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.297877\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.083794\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.060342\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.151527\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.041664\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.238147\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.051863\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.003733\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.561767\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.285884\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.070941\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.096760\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.130630\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.072113\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.128302\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.297147\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.071060\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.462345\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.411291\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.030749\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.259056\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.153767\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.101206\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.112613\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 0.109942\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 0.011562\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.180184\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.192282\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.063981\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.183987\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.023090\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.034498\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.017595\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.074273\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.006272\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.018318\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.021432\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.041001\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.004850\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.042492\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.045360\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.134490\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.045699\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.258024\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 1.016361\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.023027\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.114718\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.058729\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.024923\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.496159\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.133603\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.032151\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.287298\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.033582\n",
      "Train Epoch: 200 [0/300 (0%)]\tLoss: 1.136925\n",
      "Train Epoch: 200 [10/300 (3%)]\tLoss: 2.050002\n",
      "Train Epoch: 200 [20/300 (7%)]\tLoss: 0.583687\n",
      "Train Epoch: 200 [30/300 (10%)]\tLoss: 0.218123\n",
      "Train Epoch: 200 [40/300 (13%)]\tLoss: 0.929685\n",
      "Train Epoch: 200 [50/300 (17%)]\tLoss: 0.319206\n",
      "Train Epoch: 200 [60/300 (20%)]\tLoss: 0.405970\n",
      "Train Epoch: 200 [70/300 (23%)]\tLoss: 0.565478\n",
      "Train Epoch: 200 [80/300 (27%)]\tLoss: 0.148521\n",
      "Train Epoch: 200 [90/300 (30%)]\tLoss: 0.479907\n",
      "Train Epoch: 200 [100/300 (33%)]\tLoss: 0.577964\n",
      "Train Epoch: 200 [110/300 (37%)]\tLoss: 0.812307\n",
      "Train Epoch: 200 [120/300 (40%)]\tLoss: 0.054488\n",
      "Train Epoch: 200 [130/300 (43%)]\tLoss: 0.259853\n",
      "Train Epoch: 200 [140/300 (47%)]\tLoss: 0.234872\n",
      "Train Epoch: 200 [150/300 (50%)]\tLoss: 0.564336\n",
      "Train Epoch: 200 [160/300 (53%)]\tLoss: 0.166815\n",
      "Train Epoch: 200 [170/300 (57%)]\tLoss: 0.713431\n",
      "Train Epoch: 200 [180/300 (60%)]\tLoss: 0.269483\n",
      "Train Epoch: 200 [190/300 (63%)]\tLoss: 0.364286\n",
      "Train Epoch: 200 [200/300 (67%)]\tLoss: 0.482470\n",
      "Train Epoch: 200 [210/300 (70%)]\tLoss: 0.418678\n",
      "Train Epoch: 200 [220/300 (73%)]\tLoss: 0.131157\n",
      "Train Epoch: 200 [230/300 (77%)]\tLoss: 0.208594\n",
      "Train Epoch: 200 [240/300 (80%)]\tLoss: 0.273572\n",
      "Train Epoch: 200 [250/300 (83%)]\tLoss: 0.249365\n",
      "Train Epoch: 200 [260/300 (87%)]\tLoss: 0.532430\n",
      "Train Epoch: 200 [270/300 (90%)]\tLoss: 0.216821\n",
      "Train Epoch: 200 [280/300 (93%)]\tLoss: 0.372769\n",
      "Train Epoch: 200 [290/300 (97%)]\tLoss: 0.441296\n"
     ]
    }
   ],
   "source": [
    "for j in range(config.CLIENTS_PER_ROUND + 1):\n",
    "    server.update_nn_parameters(old_params)\n",
    "    server.test()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    print(\"Original\", recall, precision, accuracy)\n",
    "    file_name = './results/ex3/MNIST/3_8/2_run_shap_values_{}_poisoned_clients_alpha_3_8.pdf'.format(j)\n",
    "    config.POISONED_CLIENTS = j\n",
    "    experiment_util.update_configs(client_plane, server, config, observer_config)\n",
    "    client_plane.poison_clients()\n",
    "    clean_clients = experiment_util.select_random_clean(client_plane, config, config.CLIENTS_PER_ROUND - j)\n",
    "    poisoned_clients = experiment_util.select_poisoned(client_plane, j)\n",
    "    clients = [*clean_clients, *poisoned_clients]\n",
    "    print(clients)\n",
    "    experiment_util.run_round_with(clients, old_params, client_plane, server, 200)\n",
    "    server.test()\n",
    "    server_shap = server.get_shap_values()\n",
    "    recall, precision, accuracy = server.analize_test()\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "    visualizer.plot_shap_values(server_shap,file_name)\n",
    "    print(recall, precision, accuracy)\n",
    "    print(\"Poisoned clients: {}\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567554ad-b82d-4fca-975a-c32c2b46c8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9764, 0.9754, 0.9727, 0.9529, 0.8817, 0.8767]\n",
      "[tensor([0.9939, 0.9938, 0.9738, 0.9812, 0.9766, 0.9731, 0.9770, 0.9669, 0.9610,\n",
      "        0.9643]), tensor([0.9929, 0.9903, 0.9806, 0.9693, 0.9633, 0.9798, 0.9823, 0.9523, 0.9733,\n",
      "        0.9693]), tensor([0.9929, 0.9903, 0.9758, 0.9218, 0.9776, 0.9854, 0.9823, 0.9640, 0.9723,\n",
      "        0.9653]), tensor([0.9949, 0.9912, 0.9671, 0.7287, 0.9817, 0.9843, 0.9812, 0.9737, 0.9661,\n",
      "        0.9623]), tensor([0.9929, 0.9921, 0.9826, 0.0307, 0.9776, 0.9765, 0.9812, 0.9562, 0.9702,\n",
      "        0.9653]), tensor([0.9929, 0.9930, 0.9641, 0.0000, 0.9786, 0.9630, 0.9833, 0.9650, 0.9743,\n",
      "        0.9604])]\n"
     ]
    }
   ],
   "source": [
    "# Run 0\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5c5688-1d89-477c-9ca8-d6307bda0f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9761, 0.9562, 0.9728, 0.9531, 0.8871, 0.8793]\n",
      "[tensor([0.9929, 0.9894, 0.9748, 0.9673, 0.9735, 0.9809, 0.9791, 0.9689, 0.9702,\n",
      "        0.9633]), tensor([0.9929, 0.9921, 0.9758, 0.7653, 0.9786, 0.9787, 0.9854, 0.9660, 0.9702,\n",
      "        0.9584]), tensor([0.9929, 0.9903, 0.9816, 0.9168, 0.9695, 0.9843, 0.9833, 0.9650, 0.9754,\n",
      "        0.9693]), tensor([0.9918, 0.9921, 0.9758, 0.7366, 0.9766, 0.9798, 0.9854, 0.9640, 0.9723,\n",
      "        0.9584]), tensor([0.9918, 0.9885, 0.9729, 0.0871, 0.9735, 0.9787, 0.9854, 0.9640, 0.9733,\n",
      "        0.9643]), tensor([0.9918, 0.9894, 0.9758, 0.0000, 0.9837, 0.9865, 0.9760, 0.9611, 0.9764,\n",
      "        0.9623])]\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0240c2c-e2b4-4d04-8162-977e2e32555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213b7bb-a2b7-4abf-b7e2-b6c1875e376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c778c-df08-463a-963b-53cdf9edf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4\n",
    "print(accuracies)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af53b1-4d1f-458d-aa3c-c119135fd323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FashionMNIST\n",
    "For Fashion-MNIST we experiment with \n",
    "(1) 5: sandal → 4: coat,\n",
    "(2) 2: pullover → 3: shirt, and \n",
    "(3) 4: coat → 6: dress.\n",
    "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker',  'Bag', 'Ankle Boot']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
