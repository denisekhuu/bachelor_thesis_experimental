{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91c5a86-2173-4938-b072-60b6b688e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_learning.dataloader import MNISTDataloader\n",
    "from federated_learning.configuration import Configuration\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f3037c-68f9-4a6d-af05-97b8aba74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6f3d4e-5db9-4b39-80ee-23b0429a2f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1274401d70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration \n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e6d65b2-1ccf-444b-8bd8-3646431804cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST training loader loaded.\n",
      "MNIST test loader loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Configuration()\n",
    "mnist = MNISTDataloader(config)\n",
    "mnist.train_dataloader.dataset.targets[mnist.train_dataloader.dataset.targets == 0] = 1\n",
    "examples = enumerate(mnist.train_dataloader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfbc5dde-946d-4eb7-912c-f795032455f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b0095f-268d-4547-a439-0db4b9dd722c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcea48-257a-4b31-8f64-85c8f56fa051",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93547285-24f5-4945-91da-19242a7e1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c8f6f4-dcd0-4ecf-acf3-33d05518a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310f1903-285c-4320-ad7c-efb84f65649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10292430-56cb-4518-9d0e-022c75e60d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(mnist.train_dataloader.dataset) for i in range(n_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d179f4d-4d8e-4c25-8437-591f9773fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Funktions\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(mnist.train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(mnist.train_dataloader.dataset),100. * batch_idx / len(mnist.train_dataloader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(mnist.train_dataloader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "\n",
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in mnist.test_dataloader:\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            total += 1\n",
    "            print(test_loss, loss, total)\n",
    "    test_loss /= total\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(mnist.test_dataloader.dataset), 100. * correct / len(mnist.test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd299bbf-63fd-40af-a08e-42e931300747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16730/161431047.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.303955316543579 tensor(2.3040) 1\n",
      "4.606653690338135 tensor(2.3027) 2\n",
      "6.912424087524414 tensor(2.3058) 3\n",
      "9.216757535934448 tensor(2.3043) 4\n",
      "11.520921468734741 tensor(2.3042) 5\n",
      "13.827752590179443 tensor(2.3068) 6\n",
      "16.132691860198975 tensor(2.3049) 7\n",
      "18.438018560409546 tensor(2.3053) 8\n",
      "20.742637157440186 tensor(2.3046) 9\n",
      "23.050342559814453 tensor(2.3077) 10\n",
      "\n",
      "Test set: Avg. loss: 2.3050, Accuracy: 1241/10000 (12%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.317222\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.295995\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.319038\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.278315\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.263477\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.268194\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.258844\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.238255\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.215703\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.220161\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.181165\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.270455\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.196095\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.144304\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.153218\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.238630\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.089408\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.228475\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.111659\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.086368\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.184851\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.099925\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.125718\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.076559\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.152392\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.162522\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.043364\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.008768\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.947821\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.961812\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.925146\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 1.831267\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.778211\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.695556\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.530479\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.525229\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.739738\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.689988\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.373945\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.376068\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.382163\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.337467\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.259869\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.189486\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.033580\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.161686\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.007091\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.147965\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.151123\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.271316\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.017938\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.811642\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.827012\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.905209\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.962012\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.025436\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.897201\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.874314\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.931501\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.936537\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.953366\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.690497\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.856311\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.793309\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.842242\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.877043\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.912838\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.965217\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.948048\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.760626\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.758209\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.151046\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.943108\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.765957\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.791745\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.857261\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.512661\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.559115\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.660551\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.813984\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.889423\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.704092\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.415518\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.805800\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.561121\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.442142\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.749322\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.798089\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.742190\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.648130\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.868928\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.437457\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.370360\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.324972\n",
      "1.4799365997314453 tensor(1.4799) 1\n",
      "3.10859751701355 tensor(1.6287) 2\n",
      "4.763180136680603 tensor(1.6546) 3\n",
      "6.496158838272095 tensor(1.7330) 4\n",
      "8.122036933898926 tensor(1.6259) 5\n",
      "9.917187094688416 tensor(1.7952) 6\n",
      "11.741827607154846 tensor(1.8246) 7\n",
      "13.402281045913696 tensor(1.6605) 8\n",
      "15.12861454486847 tensor(1.7263) 9\n",
      "16.87597393989563 tensor(1.7474) 10\n",
      "\n",
      "Test set: Avg. loss: 1.6876, Accuracy: 7943/10000 (79%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.659837\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.567953\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.633529\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.674734\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.582556\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.751198\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.536503\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.567621\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 1.015036\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.610863\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.553076\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.541778\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.567678\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.563069\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.492641\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.836571\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.728203\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.512293\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.898733\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.614179\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.463347\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.422652\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.638622\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.972201\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.560448\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.924689\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.658678\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.489319\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.451949\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.505370\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.476107\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.453281\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.506630\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.574592\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.306711\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.498363\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.762837\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.755878\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.421385\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.751551\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.737318\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.503564\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.653153\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.315317\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.565051\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.480508\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.474334\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.630987\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.713244\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.656911\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.583204\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.529595\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.426741\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.364845\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.605056\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.708264\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.483385\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.399589\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.607316\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.673571\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.474149\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.279773\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.434184\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.458689\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.472434\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.467726\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.546382\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.694292\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.497508\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.572325\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.562530\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.696845\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.734593\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.597433\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.449502\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.553328\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.296919\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.372511\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.461526\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.660936\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.516824\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.522827\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.353588\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.576569\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.386395\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.375729\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.356924\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.491285\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.446197\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.500583\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.735755\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.341609\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.177950\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.189944\n",
      "1.522013783454895 tensor(1.5220) 1\n",
      "3.1935397386550903 tensor(1.6715) 2\n",
      "4.891065359115601 tensor(1.6975) 3\n",
      "6.688664555549622 tensor(1.7976) 4\n",
      "8.373300194740295 tensor(1.6846) 5\n",
      "10.29239571094513 tensor(1.9191) 6\n",
      "12.220062017440796 tensor(1.9277) 7\n",
      "13.892531752586365 tensor(1.6725) 8\n",
      "15.71916949748993 tensor(1.8266) 9\n",
      "17.48323881626129 tensor(1.7641) 10\n",
      "\n",
      "Test set: Avg. loss: 1.7483, Accuracy: 8385/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.388833\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.373287\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.443505\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.383761\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.339939\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.400126\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.322678\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.565131\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.560495\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.390873\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.553120\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.386640\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.409299\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.365198\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.428794\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.543284\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.733987\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.366338\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.580339\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.591702\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.276982\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.328731\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.528499\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.926652\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.456301\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.657073\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.608328\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.542246\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.350269\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.401290\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.600236\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.379030\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.381075\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.454241\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.179337\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.482474\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.466386\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.871884\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.246320\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.401525\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.398839\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.496033\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.348060\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.258560\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.465279\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.390760\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.394850\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.517497\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.398334\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.676125\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.497289\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.401194\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.286123\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.286498\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.326806\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.582574\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.467028\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.430131\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.487288\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.476576\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.243865\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.293334\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.369696\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.494294\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.568242\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.488792\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.533676\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.624963\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.622445\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.493187\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.481154\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.790825\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.423712\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.543221\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.470165\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.401398\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.275337\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.376424\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.346703\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.485576\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.418722\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.241818\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.235677\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.378153\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.255837\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.336762\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.580843\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.438093\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.482188\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.277381\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.472865\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.373869\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.178669\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.236205\n",
      "1.5905156135559082 tensor(1.5905) 1\n",
      "3.3228111267089844 tensor(1.7323) 2\n",
      "5.090831995010376 tensor(1.7680) 3\n",
      "6.9592626094818115 tensor(1.8684) 4\n",
      "8.69933032989502 tensor(1.7401) 5\n",
      "10.73389744758606 tensor(2.0346) 6\n",
      "12.74597954750061 tensor(2.0121) 7\n",
      "14.503505110740662 tensor(1.7575) 8\n",
      "16.44967293739319 tensor(1.9462) 9\n",
      "18.31147062778473 tensor(1.8618) 10\n",
      "\n",
      "Test set: Avg. loss: 1.8311, Accuracy: 8492/10000 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
